---
published: false
title: Green Tomato Soups are Poisonous
date: 2024-06-15 00:00:00 -500
categories: [statistics,research]
tags: [statistics,interpretability]
math: true
---

# What's this all about?

This post is about a totally random troll experiment about using incredibly small models to learn bi-modal data distributions. 

There's quite a bunch of research on the idea of **superposition** in deep models, where a model has to learn to represent a greater number of features than dimensions it has in its feature space. This 'feature' space is usually the residual stream of transformer models, or the information bottleneck (smallest) layer of a simpler model. This means that a neuron (or 'dimension' of a feature space) on average is involved in the encoding of more than 1 meaning. 

Again, referencing a paper that I absolutely love (Anthropic's [Toy Models of Superpositions](https://transformer-circuits.pub/2022/toy_model/index.html)), these 'meanings' are usually represented as directions in vector space. The greater an input's latent vector is in that direction (the greater the dot product), the more of that 'meaning' it has. 

I started questioning this assumption. Deep models are basically just heaps of linear transformations peppered with non-linearities. I like to use ReLU in my thought experiments because it's very simple: linear in one quadrant and 0 in all other quadrants. In the spaces where the ReLU functions do not kill any activations (set it to 0), the transformation between one layer and the next is linear. Therefore, there is a linear mapping between one concept in layer `n`, and another concept in layer `n + 1` (in that regime of space). Perhaps, if you had a direction in layer `n`'s output space that mapped to the concept of <span style='color:red'>redness</span> of a data sample (e.g. an image), then layer `n + 1` may have learnt a concept of üçÖ tomato-soup-ness that essentially looks at that particular direction in layer `n`'s output space that represents <span style='color:red'>redness</span>.

So then, I started wondering about this toy problem. Say I'm tasked with training a model that can tell by the description of a picture of a certain food item, whether the food item is poisonous. Suppose further that all the data that I'm ever going to have to worry about are either pictures of üçÖ tomato soup (which are ALL poisonous), or üßÄ cheese (which are ALL non-poisonous). And instead of pictures, I'm only given 3 numerical descriptions of the pictures ('redness', 'wetness', and 'cyanide-ness'). I know this incredibly contrived, but it's useful to imagine that perhaps you're a small portion of a CNN in the later layers that works off of features that are generated by earlier layers of the CNN, and it's your job to do the classification, and you only have 2 hidden neurons.

It just so happens that you've efficiently learnt 2 intermediate concepts with those 2 hidden neurons, such that this is your implicit decision tree:

<img src = "../../images/tomato_soup/decision_tree.png" alt="Decision Tree" width="70%" style="display:block;margin-left:auto;margin-right:auto;">
