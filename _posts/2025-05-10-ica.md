---
published: true
title: Interpretations of ICA
date: 2025-05-10 00:00:00 -500
categories: [statistics]
tags: [statistics]
math: true
---

Thinking about how Sparse Auto-encoders (SAEs) aim to learn a sparse over-complete basis (where you are trying to triangulate a larger number of sourecs than you have signals) got me thinking about Independent Component Analysis again. In particular, I wanted to see if I could articulate a mapping between ICA and SAEs. This would provide a more mathematical framework for thinking about SAEs. I do so in this post by walking through these papers:

- [Lewicki and Sejnowski](https://papers.nips.cc/paper_files/paper/1997/file/489d0396e6826eb0c1e611d82ca8b215-Paper.pdf)

# Lewicki and Sejnowski

This paper investigates "using a Laplacian prior" to learn "representations that are sparse and \[a\] non-linear function of the data" as a "method for blind source separation of fewer mixtures than sources".

## Motivations

- Overcomplete representations "(i.e. more basis vectors than input variables), can provide a better representation, because the basis vectors can be specialized for a larger variety of features present in the entire ensemble of data"
- But, "a criticism of overcomplete representations is that they are redundant, i.e. a given data point may have many possible representations"
  - This paper is about imposing a prior probability of the basis coefficients which "specifies the *probability* of the alternative representations". I'm not sure I understand this for now.

## Problem Setup

$$
\begin{align*}
x & = As + \varepsilon
\end{align*}
$$

where $A \in \mathbb{R}^{m \times n}, m < n$. $x$ is the observation (short vector), while $s$ is the hidden "true" source signal vector. Further, they assume "Gaussian additive noise so that $\log P(x \mid A, s) \propto - \lambda (x - As)^2 / 2$." What a cumbersome way of simplying saying:

$$
\varepsilon \sim \mathcal{N} (0, \lambda ')
$$

They further define a "density for the basis coefficients, $P(s)$, which specifies the *probability* of the alternative representations. The most probable representation, $\hat{s}$, is found by maximizing the posterior distribution:"

$$
\hat{s} = \max_s P(s \mid A, x) = \max_s P(s) P(x \mid A, s)
$$

They really do need an editor for this paper. Firstly, they mean to say:

$$
\hat{s} = \text{argmax}_s P(s \mid A, x) = \text{argmax}_s P(s) P(x \mid A, s)
$$

And I should note that this is simply Bayes' rule being applied. In this case, $P(s)$, a.k.a the prior, has been hydrated out and assumed to be Laplacian.

## Approaches considered

They considered these optimization approaches:
- Find $\text{argmax}_s P(s \mid A, x)$ by using the gradient of the log posterior ($\log P(s \mid A, x)$).
- Use linear programming methods to find $A$ and $s$ to maximize $\text{argmax}_s P(s \mid A, x)$ while minimizing $\mathbf{1}^\top s = \|s\|_1$. This is exactly equivalent to the objective of SAEs.

## Learning Objective

**"The learning objective is to adapt $A$ to maximize the probability of the data which is computed by marginalizing over the internal state:"**

$$
\begin{align*}
P(x \mid A) & = \int P(s) P(x \mid A, s) \text{ } ds
\end{align*}
$$

> This, I understand. A helpful note is that $s$ is distributed around some mean ($\hat{s}$), and that distribution is usually Gaussian, but in this paper, they propose for it to be Laplacian. Remember also that while $s \sim \text{Laplacian}$, the noise $\varepsilon$ in the data is still Gaussian.

They continue: "this integral cannot be evaluated analytically but can be approximated with a Gaussian integral (hence why it's usually Gaussian) around $\hat{s}$, yielding:"

$$
\begin{align*}
\log P(x \mid A) \approx \text{const.} + \log P(\hat{s}) - \frac{\lambda}{2}(x - A\hat{s})^2 - \frac{1}{2} \log \text{det} H
\end{align*}
$$

"where $H$ is the Hessian of the log posterior at $\hat{s}$." This, I did not understand, but I was able to trace through the derivation with Claude's help and so I'll write it down before it's lost once again to the ether.

### Derivation of Approximation

First, we denote the log of the integrand as $f(s)$:

$$
\begin{align*}
f(s) & = \log P(s) + \log P(x \mid A) \\
\therefore P(x \mid A) & = \int e^{f(s)} \text{ } ds
\end{align*}
$$

We know that the mean of a Gaussian (and Laplacian for that matter) distribution has the maximum probability density. This is a useful fact about $\hat{s}$, which we will try to incorporate by expressing $f(s)$ in terms of $f(s')$ using the Taylor expansion:

$$
\begin{align*}
f(s) & \approx f(\hat{s}) + \nabla f(\hat{s})^\top \left(s - \hat{s} \right) + \frac{1}{2} \left( s - \hat{s} \right)^\top H \left( s  - \hat{s}\right)
\end{align*}
$$

Since, by Bayes rule,

$$
\begin{align*}
\log P (s \mid x, A) & = \log P(s) + \log P(x \mid A, s) - \log P(x \mid A), \\
\Rightarrow \log P(s \mid x, A) & \propto \log P(s) + \log P(x \mid A, s) = f(s)
\end{align*}
$$

Since $\hat{s} = \text{argmax}_s P(s \mid x, A)$, this also means that $f(s)$ is maximized at $\hat{s}$. Hence, $\nabla f(\hat{s})$ at $\hat{s}$ is $0$. Therefore:

$$
f(s) \approx f(\hat{s}) + \frac{1}{2}\left( s - \hat{s} \right)^\top H \left(s - \hat{s} \right)
$$

Substituting back into the integral, we have:

$$
\begin{align*}
P(x \mid A) & \approx \int e^{f(\hat{s})} e^{\frac{1}{2}(s - \hat{s})^\top H (s - \hat{s})} \text{ } ds \\
& = e^{f(\hat{s})} \int e^{-\frac{1}{2}(s - \hat{s})^\top K (s - \hat{s})} \text{ } ds
\end{align*}
$$

Where $K = -H$. Crucially, the second term is a Gaussian integral, with a known solution below.Note that because $H$ is the Hessian of a concave quadratic, $H$ is negative definite, and has a positive determinant if $s$ is even-dimensional, and negative determinant if $s$ is odd-dimensional. Since we're using $K = -H$, this is not a problem anymore because $K$ is positive semidefinite and has a non-negative determinant.

$$
\begin{align*}
\int e^{-\frac{1}{2}(s - \hat{s})^\top K (s - \hat{s})} \text{ } ds & = \sqrt{\frac{(2 \pi)^d}{| K |}}
\end{align*}
$$

Where $\|K \|$ is the determinant of $K$. Therefore:

$$
\begin{align*}
P(x \mid A) & \approx e^{f(\hat{s})} \int e^{-\frac{1}{2}(s - \hat{s})^\top K (s - \hat{s})} \text{ } ds \\
& = e^{f(\hat{s})} (2 \pi)^\frac{d}{2} \cdot | K | ^{-\frac{1}{2}} \\
\Rightarrow \log P(x \mid A) & = f (\hat{s}) + \frac{d}{2} \log (2 \pi) - \frac{1}{2} \log |K| \\
& = \log P(\hat{s}) + \log P(x \mid A, \hat{s}) + \frac{d}{2} \log (2 \pi) - \frac{1}{2} \log |K| \\
\end{align*}
$$

At this point, we pretty much have what we want. We just have to note that for the $\log P(x \mid A, \hat{s})$ term, since Gaussian PDF is given by some scalar multiple of $\exp(-(x - \mu)^2)$, we simply have:

$$
\begin{align*}
\log P(x \mid A, s) = -k \left( x - A s \right)^2 + \text{const}
\end{align*}
$$

And further noting that the $\frac{d}{2} \log (2 \pi)$ term is a constant, we have our approximation:

$$
\begin{align*}
\log P(x \mid A) \approx \text{const} + \log P (\hat{s}) - k(x - A\hat{s})^2 - \frac{1}{2} \log |K|
\end{align*}
$$

Where $\|K\|$ is explicitly the log determinant of $\text{abs}(H)$, and not the determinant of $H$, as the paper suggests.

## Learning Rule

